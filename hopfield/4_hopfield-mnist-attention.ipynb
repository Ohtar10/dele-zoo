{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield - Self-attention\n",
    "The update of the new energy function (notebook [3_hopfield-continuous-value.ipynb](./3_hopfield-continuous-value.ipynb)) is the self-attention of transformer networks.\n",
    "\n",
    "References:\n",
    "* https://ml-jku.github.io/hopfield-layers/#update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From equation:\n",
    "$$\n",
    "\\xi^{new} = X\\mathrm{softmax}(\\beta X^T \\xi)\n",
    "$$\n",
    "\n",
    "For $S$ state patterns $\\Xi=(\\xi_1,...,\\xi_s)$, the equation can be generalized to:\n",
    "$$\n",
    "\\Xi^{\\mathrm{new}} = X\\mathrm{softmax}(\\beta X^T\\Xi)\n",
    "$$\n",
    "\n",
    "Where $X^T$ can be considered as $N$ *raw **stored** patterns* $Y=(y_1,...y_N)^T$, which are mapped to an associative space via $W_K$, and $\\Xi^T$ as $S$ *raw **state** patterns* $R=(\\xi_1,...,\\xi_S)^T$, which are mapped to an associative space via $W_Q$.\n",
    "\n",
    "Then, by setting:\n",
    "$$\n",
    "Q = \\Xi^T = RW_Q \\\\\n",
    "K = X^T = YW_K \\\\\n",
    "\\beta = \\frac{1}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "we obtain:\n",
    "$$\n",
    "(Q^{\\mathrm{new}})^T = K^T \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}KQ^T)\n",
    "$$\n",
    "\n",
    "Where $W_Q$ and $W_K$ are matrices which map the respective patterns into the associative space. In the previous equation, the softmax is applied column-wise tot he matrix $KQ^T$. By transposing the equation, which also means softmax is now applied row-wise to its transposed input $QK^T$, we obtain:\n",
    "\n",
    "$$\n",
    "(Q^{\\mathrm{new}})^T = \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}QK^T)K\n",
    "$$\n",
    "\n",
    "Now, by projecting $Q^{new}$ via another projection matrix $W_V$ we obtain:\n",
    "\n",
    "$$\n",
    "Z = Q^{new}W_V = \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}QK^T)KW_V = \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}QK^T)V\n",
    "$$\n",
    "\n",
    "Which is basically the transformer attention formula (As per Attention is All you need):\n",
    "$$\n",
    "\\mathrm{Attention(Q, K, V)} = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "Some remarks:\n",
    "* Transformer based models usually implement embedding layers before the attention mechanism, i.e., what is feed into the attention mechanism is an embedding of the input/outputs.\n",
    "    * These embeddings have trainable matrices that produces them during training.\n",
    "* In the new Hopfield definition, the matrices that produce the embeddings are explicit in the formula, i.e., matrices $W_Q$, $W_K$, and $W_V$ are the matrices that transform the input/outpus into the associative space that is feed to the attention mechanism.\n",
    "* One differencing aspect of original attention vs Hopfield is the value of $\\beta$ parameter. Original attention fixes this to be dependent on the dimension of the embeddings, which for large values of $d_k$ will yield in smaller $\\beta$, which in turn, as per explained in the new Hopfield paper, means the retrievals will tend to be metastable states or the average of similar patterns which can give us an intuition of why they work and why the concept of \"Attention\".\n",
    "* The new Hopfield definition can be interpreted as a generalization of the attention mechanism.\n",
    "* The result of the retrieval, which is the attention produced from the state patterns against the stored patterns, can be the input to fully connected layers for some classification task.\n",
    "* Similarly, before the attention mechanism, there can be other feature extraction layers s.a. CNNs that will produce vectors for which store/retrieval process can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopfield MNIST #1 - Predict using full patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random_seed = 1234\n",
    "train_split_fraction = 0.7\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = MNIST(\n",
    "    './mnist-train', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=data_transform,\n",
    "    )\n",
    "\n",
    "test_set = MNIST(\n",
    "    './mnist-test', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=data_transform,\n",
    "    )\n",
    "\n",
    "train_set_size = len(train_set)\n",
    "indices = list(range(train_set_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(train_split_fraction * train_set_size))\n",
    "stored_patterns_idx, train_idx = indices[split:], indices[:split]\n",
    "\n",
    "stored_patterns_sampler = SubsetRandomSampler(stored_patterns_idx)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=2048, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=2048, shuffle=True, num_workers=4)\n",
    "stored_patterns_loader = DataLoader(train_set, batch_size=split, sampler=stored_patterns_sampler)\n",
    "stored_patterns = list(stored_patterns_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def prepare_device(obj, use_cuda: bool = True):\n",
    "    \"\"\"Prepare device.\n",
    "    Moves the passed object to the appropriate\n",
    "    device.\"\"\"\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        return obj.to(\"cuda\")\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Hopfield network definition representing the generalized attention mechanism:\n",
    "$$\n",
    "Z = \\mathrm{softmax}(\\beta \\cdot RW_QW_K^TY^T)YW_KW_V\n",
    "$$\n",
    "\n",
    "Where $R$ are the raw state patterns, and $Y$ are the raw stored patterns. Notice that $V$ is represented by $YW_KW_V$ in this case. But the key point is that the model is flexible and either some, all or none of the elements can be trainable ($W$ matrices associated with each element.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class HopfieldNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        store_dim: int, \n",
    "        hidden_store_dim: int, \n",
    "        state_dim: int, \n",
    "        hidden_state_dim: int, \n",
    "        value_dim: int,\n",
    "        hidden_value_dim: int,\n",
    "        dropout_ratio: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.store_dim = store_dim\n",
    "        self.hidden_store_dim = hidden_store_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_state_dim = hidden_state_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.hidden_value_dim = hidden_value_dim\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.device = device\n",
    "\n",
    "        self.__init_parameters()\n",
    "    \n",
    "    def __init_parameters(self):\n",
    "        # state patterns\n",
    "        self.WQ = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_state_dim),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "        # stored patterns\n",
    "        self.WK = nn.Sequential(\n",
    "            nn.Linear(self.store_dim, self.hidden_store_dim),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "        # value patterns\n",
    "        self.WV = nn.Sequential(\n",
    "            nn.Linear(self.value_dim, self.hidden_value_dim),\n",
    "            nn.Dropout(self.dropout_ratio)\n",
    "        )\n",
    "\n",
    "    def to(self, device: str):\n",
    "        super().to(device)\n",
    "        self.WQ = self.WQ.to(device)\n",
    "        self.WK = self.WK.to(device)\n",
    "        self.WV = self.WV.to(device)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def forward(self, state_patterns, stored_patterns, value_patterns=None, beta=1.0):\n",
    "        # Normalize inputs to make it work\n",
    "        Q = state_patterns / torch.norm(state_patterns, dim=1, keepdim=True)\n",
    "        K = stored_patterns / torch.norm(stored_patterns, dim=1, keepdim=True)\n",
    "\n",
    "        # RW_Q\n",
    "        Q = self.WQ(Q)\n",
    "        # W_K^TY^T\n",
    "        K = self.WK(K)\n",
    "        # VW_V\n",
    "        V = self.WV(value_patterns)\n",
    "        # Log softmax because we later use CrossEntropyLoss\n",
    "        # which expects logits because it will apply a softmax\n",
    "        # internally. (Pytorch thing)    \n",
    "        Z = torch.log_softmax(beta * Q @ K.T, dim=1) @ V\n",
    "        \n",
    "        return Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in model (W params + bias): 100590\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "HopfieldNet                              --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Linear: 2-1                       50,240\n",
      "│    └─Dropout: 2-2                      --\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─Linear: 2-3                       50,240\n",
      "│    └─Dropout: 2-4                      --\n",
      "├─Sequential: 1-3                        --\n",
      "│    └─Linear: 2-5                       110\n",
      "│    └─Dropout: 2-6                      --\n",
      "=================================================================\n",
      "Total params: 100,590\n",
      "Trainable params: 100,590\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "HopfieldNet(\n",
      "  (WQ): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (WK): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (WV): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary as torch_summary\n",
    "\n",
    "store_dim = 784 # 784 = 28 * 28, i.e., flattened images\n",
    "state_dim = store_dim # stored and state paterns have the same dim\n",
    "value_dim = 10 # the one-hot expected label from the state patterns\n",
    "hidden_store_dim, hidden_state_dim = 64, 64\n",
    "hidden_value_dim = value_dim # We don't need to embeed in a lower dimension the one hot encoding\n",
    "\n",
    "model = HopfieldNet(\n",
    "    store_dim, \n",
    "    hidden_store_dim, \n",
    "    state_dim, \n",
    "    hidden_state_dim, \n",
    "    value_dim, \n",
    "    hidden_value_dim,\n",
    "    dropout_ratio=0.2)\n",
    "model = prepare_device(model)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of parameters in model (W params + bias): {num_params}\")\n",
    "print(torch_summary(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape operations smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Result: torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "sim_raw_state = torch.randn((20, 784)).to(device)\n",
    "sim_raw_stored = torch.randn((200, 784)).to(device)\n",
    "sim_raw_value = torch.randn((200, 10)).to(device)\n",
    "result = model(sim_raw_state, sim_raw_stored, sim_raw_value)\n",
    "print(f\"Shape of Result: {result.shape}\")\n",
    "assert list(result.shape) == [20, 10], \"Result shape doesn't match with the expected result\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss, optimizer and evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "metric = torchmetrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training & Evaluation loop. During train/eval, a fixed amount of the input batch will be used as true training data and the rest will be used as raw store patterns with their true labels as stored values. The forward pass will produce a vector with the expected one-hot encoded shape and evaluated against the true trainind data labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class Context:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def train_step(\n",
    "    ctx: Context,\n",
    "    loss_fn, \n",
    "    optimizer):\n",
    "    \"\"\"Train step.\n",
    "    Traverses the dataloader once\n",
    "    and performs updates on each\n",
    "    bach.\"\"\"\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    for R, y in ctx.dataloader:\n",
    "        total_batches += 1\n",
    "        batch = prepare_device(R).view(-1, 28 * 28)\n",
    "        y = prepare_device(y)\n",
    "        y_state = y[:ctx.split_size]\n",
    "        y_store = y[ctx.split_size:]\n",
    "        state_patterns = batch[:ctx.split_size]\n",
    "        store_patterns = batch[ctx.split_size:]\n",
    "        value_patterns = F.one_hot(y_store, num_classes=10).to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(state_patterns, store_patterns, value_patterns, beta=beta)\n",
    "        loss = loss_fn(pred, y_state)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        log_gradients(ctx.logger, model, ctx.step)\n",
    "\n",
    "    total_size = len(ctx.dataloader.dataset)\n",
    "    # On every batch, we took split_size examples as store patterns\n",
    "    true_train_size = total_size - ctx.split_size * total_batches\n",
    "    return total_loss / true_train_size\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    ctx,\n",
    "    metric_fn):\n",
    "    \"\"\"Evaluate.\n",
    "    Calculates the metric on the provided \n",
    "    dataloader\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for R, y in ctx.dataloader:\n",
    "            batch = prepare_device(R).view(-1, 28 * 28)\n",
    "            y = prepare_device(y)\n",
    "            y_state = y[:ctx.split_size]\n",
    "            y_store = y[ctx.split_size:]\n",
    "            state_patterns = batch[:ctx.split_size]\n",
    "            store_patterns = batch[ctx.split_size:]\n",
    "            value_patterns = F.one_hot(y_store, num_classes=10).to(torch.float32)\n",
    "\n",
    "            preds = model(state_patterns, store_patterns, value_patterns, beta=beta).to(\"cpu\")\n",
    "            metric = metric_fn(preds, y_state.to(\"cpu\"))\n",
    "        metric = metric_fn.compute()\n",
    "        metric_fn.reset()\n",
    "    return metric\n",
    "\n",
    "\n",
    "def log_summary(logger: SummaryWriter, loss, metric, step):\n",
    "    logger.add_scalar('loss', loss, global_step=step)\n",
    "    logger.add_scalar('accuracy', metric, global_step=step)\n",
    "    logger.flush()\n",
    "\n",
    "\n",
    "def log_gradients(logger: SummaryWriter, model, step):\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        else:\n",
    "            logger.add_histogram(\n",
    "                tag=f'grads/{name}', \n",
    "                values=p.grad.data.detach().cpu().numpy(),\n",
    "                global_step=step\n",
    "            )\n",
    "    logger.flush()\n",
    "    \n",
    "\n",
    "def train(\n",
    "    epochs, \n",
    "    dataloader, \n",
    "    model,\n",
    "    split_size, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    metric_fn,\n",
    "    beta,\n",
    "    logger: SummaryWriter):\n",
    "\n",
    "    ctx = Context(\n",
    "        dataloader=dataloader,\n",
    "        model=model,\n",
    "        split_size=split_size,\n",
    "        beta=beta,\n",
    "        logger=logger,\n",
    "        step=0\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    with tqdm(total=epochs) as progress:\n",
    "        for i in range(epochs):\n",
    "            ctx.step = i\n",
    "            loss = train_step(\n",
    "                ctx,\n",
    "                loss_fn, \n",
    "                optimizer)\n",
    "            metric = evaluate(\n",
    "                ctx,\n",
    "                metric_fn)\n",
    "\n",
    "            log_summary(logger, loss, metric, i)\n",
    "\n",
    "            progress.set_postfix({\n",
    "                        \"loss\": f\"{loss:.4f}\",\n",
    "                        \"accuracy\": f\"{metric:.2f}\"\n",
    "                    })\n",
    "            progress.update()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [42:59<00:00, 12.90s/it, loss=0.0067, accuracy=0.79]\n"
     ]
    }
   ],
   "source": [
    "# from tensorboardX import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# beta = 1 / np.sqrt(store_dim) # beta as classic transformers\n",
    "beta = 4.0\n",
    "with SummaryWriter(log_dir='./tensorboard') as tb_writer:\n",
    "    train(200, train_loader, model, 1024, loss_fn, optimizer, metric, beta, tb_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 80.20%\n"
     ]
    }
   ],
   "source": [
    "ctx = Context(\n",
    "    dataloader=test_loader,\n",
    "    model=model,\n",
    "    split_size=1024,\n",
    "    beta=beta\n",
    ")\n",
    "test_acc = evaluate(ctx, metric).item()\n",
    "print(f\"Test set accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 0\n",
    "digit = train_set[i][0].reshape(28, 28)\n",
    "label = train_set[i][1]\n",
    "plt.title(f'Label={label}')\n",
    "plt.imshow(digit, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74970075a403ac97ea2d08d0feef9384cc88f994ce83c5f27de81026cb120c7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('hopfield-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
