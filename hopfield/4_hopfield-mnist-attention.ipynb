{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield - Self-attention\n",
    "The update of the new energy function (notebook [3_hopfield-continuous-value.ipynb](./3_hopfield-continuous-value.ipynb)) is the self-attention of transformer networks.\n",
    "\n",
    "References:\n",
    "* https://ml-jku.github.io/hopfield-layers/#update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From equation:\n",
    "$$\n",
    "\\xi^{new} = X\\mathrm{softmax}(\\beta X^T \\xi)\n",
    "$$\n",
    "\n",
    "For $S$ state patterns $\\Xi=(\\xi_1,...,\\xi_s)$, the equation can be generalized to:\n",
    "$$\n",
    "\\Xi^{\\mathrm{new}} = X\\mathrm{softmax}(\\beta X^T\\Xi)\n",
    "$$\n",
    "\n",
    "Where $X^T$ can be considered as $N$ *raw **stored** patterns* $Y=(y_1,...y_N)^T$, which are mapped to an associative space via $W_K$, and $\\Xi^T$ as $S$ *raw **state** patterns* $R=(\\xi_1,...,\\xi_S)^T$, which are mapped to an associative space via $W_Q$.\n",
    "\n",
    "Then, by setting:\n",
    "$$\n",
    "Q = \\Xi^T = RW_Q \\\\\n",
    "K = X^T = YW_K \\\\\n",
    "\\beta = \\frac{1}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "we obtain:\n",
    "$$\n",
    "(Q^{\\mathrm{new}})^T = K^T \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}KQ^T)\n",
    "$$\n",
    "\n",
    "Where $W_Q$ and $W_K$ are matrices which map the respective patterns into the associative space. In the previous equation, the softmax is applied column-wise tot he matrix $KQ^T$. By transposing the equation, which also means softmax is now applied row-wise to its transposed input $QK^T$, we obtain:\n",
    "\n",
    "$$\n",
    "(Q^{\\mathrm{new}})^T = \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}QK^T)K\n",
    "$$\n",
    "\n",
    "Now, by projecting $Q^{new}$ via another projection matrix $W_V$ we obtain:\n",
    "\n",
    "$$\n",
    "Z = Q^{new}W_V = \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}QK^T)KW_V = \\mathrm{softmax}(\\frac{1}{\\sqrt{d_k}}QK^T)V\n",
    "$$\n",
    "\n",
    "Which is basically the transformer attention formula (As per Attention is All you need):\n",
    "$$\n",
    "\\mathrm{Attention(Q, K, V)} = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "Some remarks:\n",
    "* Transformer based models usually implement embedding layers before the attention mechanism, i.e., what is feed into the attention mechanism is an embedding of the input/outputs.\n",
    "    * These embeddings have trainable matrices that produces them during training.\n",
    "* In the new Hopfield definition, the matrices that produce the embeddings are explicit in the formula, i.e., matrices $W_Q$, $W_K$, and $W_V$ are the matrices that transform the input/outpus into the associative space that is feed to the attention mechanism.\n",
    "* One differencing aspect of original attention vs Hopfield is the value of $\\beta$ parameter. Original attention fixes this to be dependent on the dimension of the embeddings, which for large values of $d_k$ will yield in smaller $\\beta$, which in turn, as per explained in the new Hopfield paper, means the retrievals will tend to be metastable states or the average of similar patterns which can give us an intuition of why they work and why the concept of \"Attention\".\n",
    "* The new Hopfield definition can be interpreted as a generalization of the attention mechanism.\n",
    "* The result of the retrieval, which is the attention produced from the state patterns against the stored patterns, can be the input to fully connected layers for some classification task.\n",
    "* Similarly, before the attention mechanism, there can be other feature extraction layers s.a. CNNs that will produce vectors for which store/retrieval process can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopfield MNIST #1 - Predict using full patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random_seed = 1234\n",
    "train_split_fraction = 0.7\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = MNIST(\n",
    "    './mnist-train', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=data_transform,\n",
    "    )\n",
    "\n",
    "test_set = MNIST(\n",
    "    './mnist-test', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=data_transform,\n",
    "    )\n",
    "\n",
    "train_set_size = len(train_set)\n",
    "indices = list(range(train_set_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(train_split_fraction * train_set_size))\n",
    "stored_patterns_idx, train_idx = indices[split:], indices[:split]\n",
    "\n",
    "stored_patterns_sampler = SubsetRandomSampler(stored_patterns_idx)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1024, sampler=train_sampler, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=1024, shuffle=True, num_workers=4)\n",
    "stored_patterns_loader = DataLoader(train_set, batch_size=split, sampler=stored_patterns_sampler)\n",
    "stored_patterns = list(stored_patterns_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "def prepare_device(obj, use_cuda: bool = True):\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        return obj.to(\"cuda\")\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class HopfieldNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        store_dim: int, \n",
    "        hidden_store_dim: int, \n",
    "        state_dim: int, \n",
    "        hidden_state_dim: int, \n",
    "        value_dim: int,\n",
    "        hidden_value_dim: int,\n",
    "        learn_v: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.store_dim = store_dim\n",
    "        self.hidden_store_dim = hidden_store_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.hidden_state_dim = hidden_state_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.hidden_value_dim = hidden_value_dim\n",
    "        self.device = device\n",
    "        self.learn_v = learn_v\n",
    "\n",
    "        self.__init_parameters()\n",
    "        # self._reset_parameters()\n",
    "    \n",
    "    def __init_parameters(self):\n",
    "        # state patterns\n",
    "        self.WQ = nn.Parameter(\n",
    "            torch.Tensor(self.state_dim, self.hidden_state_dim)\n",
    "        )\n",
    "        # stored patterns\n",
    "        self.WK = nn.Parameter(\n",
    "            torch.Tensor(self.store_dim, self.hidden_store_dim)\n",
    "        )\n",
    "        # value patterns\n",
    "        if self.learn_v:\n",
    "            self.WV = nn.Parameter(\n",
    "                torch.Tensor(self.value_dim, self.hidden_value_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.WV = torch.eye(self.value_dim)\n",
    "\n",
    "    def to(self, device: str):\n",
    "        super().to(device)\n",
    "        self.WQ = self.WQ.to(device)\n",
    "        self.WK = self.WK.to(device)        \n",
    "        self.WV = self.WV.to(device)\n",
    "        return self\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.normal_(self.WQ, 0., 0.01)\n",
    "        nn.init.normal_(self.WK, 0., 0.01)\n",
    "        if self.learn_v:\n",
    "            nn.init.normal_(self.WV, 0., 0.01)\n",
    "\n",
    "    def print_shapes(self):\n",
    "        print(f\"WQ: {self.WQ.shape}\")\n",
    "        print(f\"WK: {self.WK.shape}\")\n",
    "        print(f\"WV: {self.WV.shape}\")\n",
    "        \n",
    "\n",
    "    def forward(self, state_patterns, stored_patterns, value_patterns, beta=1.0):\n",
    "        Q = state_patterns @ self.WQ\n",
    "        Q = Q / torch.norm(Q, dim=1, keepdim=True)\n",
    "        K = stored_patterns @ self.WK\n",
    "        K = K / torch.norm(K, dim=1, keepdim=True)\n",
    "        V = value_patterns @ self.WV\n",
    "\n",
    "        Z = torch.log_softmax(beta * Q @ K.T, dim=1) @ V\n",
    "        return Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in model: 50176\n"
     ]
    }
   ],
   "source": [
    "store_dim = 784 # 784 = 28 * 28, i.e., flattened images\n",
    "state_dim = store_dim # stored and state paterns have the same dim\n",
    "value_dim = 10 # the one-hot expected label from the state patterns\n",
    "hidden_store_dim, hidden_state_dim = 32, 32\n",
    "hidden_value_dim = value_dim # We don't need to embeed in a lower dimension the one hot encoding\n",
    "\n",
    "model = HopfieldNet(\n",
    "    store_dim, \n",
    "    hidden_store_dim, \n",
    "    state_dim, \n",
    "    hidden_state_dim, \n",
    "    value_dim, \n",
    "    hidden_value_dim,\n",
    "    learn_v=False)\n",
    "model = prepare_device(model)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of parameters in model: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape operations test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ: torch.Size([784, 32])\n",
      "WK: torch.Size([784, 32])\n",
      "WV: torch.Size([10, 10])\n",
      "Shape of Result: torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "model.print_shapes()\n",
    "sim_raw_state = torch.randn((20, 784)).to(device)\n",
    "sim_raw_stored = torch.randn((200, 784)).to(device)\n",
    "sim_raw_value = torch.randn((200, 10)).to(device)\n",
    "result = model(sim_raw_state, sim_raw_stored, sim_raw_value)\n",
    "print(f\"Shape of Result: {result.shape}\")\n",
    "assert list(result.shape) == [20, 10], \"Result shape doesn't match with the expected result\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "metric = torchmetrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def train_loop(\n",
    "    dataloader, \n",
    "    model, \n",
    "    stored_patterns_K, \n",
    "    stored_projections_V,\n",
    "    beta,\n",
    "    loss_fn, \n",
    "    optimizer):\n",
    "    for R, y in dataloader:\n",
    "        R = prepare_device(R).view(-1, 28 * 28)\n",
    "        y = prepare_device(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(R, stored_patterns_K, stored_projections_V, beta=beta)\n",
    "        loss = loss_fn(pred, y)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    dataloader, \n",
    "    model,\n",
    "    stored_patterns_K, \n",
    "    stored_projections_V, \n",
    "    beta,\n",
    "    metric_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for R, y in dataloader:\n",
    "            R = prepare_device(R).view(-1, 28 * 28)\n",
    "\n",
    "            preds = model(R, stored_patterns_K, stored_projections_V, beta=beta).to(\"cpu\")\n",
    "            metric = metric_fn(preds, y)\n",
    "        metric = metric_fn.compute()\n",
    "        metric_fn.reset()\n",
    "    return metric\n",
    "\n",
    "\n",
    "def log(logger: SummaryWriter, loss, metric, model, step):\n",
    "    logger.add_scalar('loss', loss, global_step=step)\n",
    "    logger.add_scalar('accuracy', metric, global_step=step)\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        else:\n",
    "            logger.add_histogram(\n",
    "                tag=f'grads/{name}', \n",
    "                values=p.grad.detach().cpu().numpy(),\n",
    "                global_step=step\n",
    "            )\n",
    "    logger.flush()\n",
    "\n",
    "def train(\n",
    "    epochs, \n",
    "    dataloader, \n",
    "    model,\n",
    "    stored_patterns_K,\n",
    "    stored_projections_V, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    metric_fn,\n",
    "    beta,\n",
    "    logger: SummaryWriter):\n",
    "    model.train()\n",
    "    with tqdm(total=epochs) as progress:\n",
    "        for i in range(epochs):\n",
    "            loss = train_loop(\n",
    "                dataloader, \n",
    "                model,\n",
    "                stored_patterns_K,\n",
    "                stored_projections_V,\n",
    "                beta,\n",
    "                loss_fn, \n",
    "                optimizer)\n",
    "            metric = evaluate(\n",
    "                dataloader, \n",
    "                model, \n",
    "                stored_patterns_K,\n",
    "                stored_projections_V,\n",
    "                beta,\n",
    "                metric_fn)\n",
    "\n",
    "            log(logger, loss, metric, model, i)\n",
    "\n",
    "            progress.set_postfix({\n",
    "                        \"loss\": f\"{loss:.2f}\",\n",
    "                        \"accuracy\": f\"{metric:.2f}\"\n",
    "                    })\n",
    "            progress.update()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = stored_patterns[0][0]\n",
    "V = stored_patterns[0][1]\n",
    "K = K.view(-1, 28*28)\n",
    "K = prepare_device(K)\n",
    "V = prepare_device(F.one_hot(V, num_classes=10)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:11<00:00,  7.15s/it, loss=2277.45, accuracy=0.09]\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# beta = 1 / np.sqrt(store_dim) # beta as classic transformers\n",
    "beta = 4.0\n",
    "with SummaryWriter(logdir='./tensorboard') as tb_writer:\n",
    "    train(10, train_loader, model, K, V, loss_fn, optimizer, metric, beta, tb_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 2\n",
    "digit = train_set[i][0].reshape(28, 28)\n",
    "label = train_set[i][1]\n",
    "# digit = K[i].cpu().numpy().reshape(28, 28)\n",
    "# label = np.argmax(K_labels[i].cpu().numpy())\n",
    "plt.title(f'Label={label}')\n",
    "plt.imshow(digit, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = train_set[i]\n",
    "test_x = prepare_device(test_x.reshape(-1, 28 * 28))\n",
    "pred_y = model(test_x, K, V, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-18006.6562, -19799.4844, -17350.2656, -17957.6719, -16595.9062,\n",
       "         -15626.0156, -17232.7031, -19015.7344, -17262.0938, -17497.2188]],\n",
       "       device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1021, 0.1123, 0.0984, 0.1018, 0.0941, 0.0886, 0.0977, 0.1078, 0.0979,\n",
       "         0.0992]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = test_x @ model.WQ\n",
    "Q = Q / torch.norm(Q, dim=1, keepdim=True)\n",
    "K_ = K @ model.WK\n",
    "K_ = K_ / torch.norm(K_, dim=1, keepdim=True)\n",
    "V = V @ model.WV\n",
    "\n",
    "Z = torch.softmax(beta * Q @ K_.T, dim=1) @ V\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18000, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18000, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model.WK.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74970075a403ac97ea2d08d0feef9384cc88f994ce83c5f27de81026cb120c7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('hopfield-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
